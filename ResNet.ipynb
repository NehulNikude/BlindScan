{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04a06e77-c59c-4397-b4dd-d9d04ae4129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d87df2d-6960-4b4f-b8ba-8b94aab639f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364989a0-7889-4e57-b3fe-19ae7452d938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3500 files belonging to 7 classes.\n",
      "Using 2800 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  r\"Dataset v5\",\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  label_mode='int',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17c35df-f9ca-444f-bb8d-21a375696a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3500 files belonging to 7 classes.\n",
      "Using 700 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  r\"Dataset v5\",\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  label_mode='int',\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fff6228-8d8a-4b38-9922-b1217e3a1ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '20', '200', '2000', '50', '500']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02185c51-b827-4c6f-8de1-d80f61d5161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping class names to integer labels\n",
    "class_to_label = {class_name: label for label, class_name in enumerate(class_names)}\n",
    "\n",
    "# Dummy labels \n",
    "labels = ['10', '100', '20', '200', '2000', '50', '500']\n",
    "integer_labels = [class_to_label[label] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af87620d-f7c9-4300-841a-6461db22da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a7f1b3d-28f3-48d2-b487-aee966b1ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.resnet.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0e13672-42aa-4aad-ba80-9676744179b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (256, 256)\n",
    "img_shape = img_size + (3,)\n",
    "base_model = tf.keras.applications.ResNet50(input_shape=img_shape,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60cbe58d-21f9-47f1-aef3-e120e1e4c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3caecb0-74ec-4630-8462-f2dfd1967b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1_pad (ZeroPadding2D)      (None, 262, 262, 3)  0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1_conv (Conv2D)            (None, 128, 128, 64  9472        ['conv1_pad[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_bn (BatchNormalization)  (None, 128, 128, 64  256         ['conv1_conv[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv1_relu (Activation)        (None, 128, 128, 64  0           ['conv1_bn[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pad (ZeroPadding2D)      (None, 130, 130, 64  0           ['conv1_relu[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1_pool (MaxPooling2D)      (None, 64, 64, 64)   0           ['pool1_pad[0][0]']              \n",
      "                                                                                                  \n",
      " conv2_block1_1_conv (Conv2D)   (None, 64, 64, 64)   4160        ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block1_0_conv (Conv2D)   (None, 64, 64, 256)  16640       ['pool1_pool[0][0]']             \n",
      "                                                                                                  \n",
      " conv2_block1_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block1_0_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block1_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
      "                                                                  'conv2_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block1_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block2_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block2_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block2_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block2_add (Add)         (None, 64, 64, 256)  0           ['conv2_block1_out[0][0]',       \n",
      "                                                                  'conv2_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block2_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_conv (Conv2D)   (None, 64, 64, 64)   16448       ['conv2_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv2_block3_1_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_1_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_2_conv (Conv2D)   (None, 64, 64, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_2_bn (BatchNormal  (None, 64, 64, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_2_relu (Activatio  (None, 64, 64, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv2_block3_3_conv (Conv2D)   (None, 64, 64, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv2_block3_3_bn (BatchNormal  (None, 64, 64, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv2_block3_add (Add)         (None, 64, 64, 256)  0           ['conv2_block2_out[0][0]',       \n",
      "                                                                  'conv2_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv2_block3_out (Activation)  (None, 64, 64, 256)  0           ['conv2_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_conv (Conv2D)   (None, 32, 32, 128)  32896       ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block1_0_conv (Conv2D)   (None, 32, 32, 512)  131584      ['conv2_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block1_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block1_0_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block1_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
      "                                                                  'conv3_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block1_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block2_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block2_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block2_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block2_add (Add)         (None, 32, 32, 512)  0           ['conv3_block1_out[0][0]',       \n",
      "                                                                  'conv3_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block2_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block3_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block3_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block3_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block3_add (Add)         (None, 32, 32, 512)  0           ['conv3_block2_out[0][0]',       \n",
      "                                                                  'conv3_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block3_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_conv (Conv2D)   (None, 32, 32, 128)  65664       ['conv3_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv3_block4_1_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_1_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_2_conv (Conv2D)   (None, 32, 32, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_2_bn (BatchNormal  (None, 32, 32, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_2_relu (Activatio  (None, 32, 32, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv3_block4_3_conv (Conv2D)   (None, 32, 32, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv3_block4_3_bn (BatchNormal  (None, 32, 32, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv3_block4_add (Add)         (None, 32, 32, 512)  0           ['conv3_block3_out[0][0]',       \n",
      "                                                                  'conv3_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv3_block4_out (Activation)  (None, 32, 32, 512)  0           ['conv3_block4_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_conv (Conv2D)   (None, 16, 16, 256)  131328      ['conv3_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block1_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block1_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block1_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block1_0_conv (Conv2D)   (None, 16, 16, 1024  525312      ['conv3_block4_out[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_0_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block1_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
      "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block1_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block1_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block2_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block2_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block2_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block2_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block2_add (Add)         (None, 16, 16, 1024  0           ['conv4_block1_out[0][0]',       \n",
      "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block2_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block2_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block3_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block3_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block3_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block3_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block3_add (Add)         (None, 16, 16, 1024  0           ['conv4_block2_out[0][0]',       \n",
      "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block3_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block3_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block3_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block4_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block4_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block4_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block4_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block4_add (Add)         (None, 16, 16, 1024  0           ['conv4_block3_out[0][0]',       \n",
      "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block4_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block4_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block4_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block5_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block5_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block5_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block5_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block5_add (Add)         (None, 16, 16, 1024  0           ['conv4_block4_out[0][0]',       \n",
      "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block5_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block5_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_1_conv (Conv2D)   (None, 16, 16, 256)  262400      ['conv4_block5_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv4_block6_1_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_1_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_2_conv (Conv2D)   (None, 16, 16, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv4_block6_2_bn (BatchNormal  (None, 16, 16, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv4_block6_2_relu (Activatio  (None, 16, 16, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv4_block6_3_conv (Conv2D)   (None, 16, 16, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_3_bn (BatchNormal  (None, 16, 16, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " conv4_block6_add (Add)         (None, 16, 16, 1024  0           ['conv4_block5_out[0][0]',       \n",
      "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv4_block6_out (Activation)  (None, 16, 16, 1024  0           ['conv4_block6_add[0][0]']       \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv5_block1_1_conv (Conv2D)   (None, 8, 8, 512)    524800      ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block1_0_conv (Conv2D)   (None, 8, 8, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block1_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block1_0_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block1_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
      "                                                                  'conv5_block1_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block1_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block1_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block2_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block2_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block2_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block2_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block1_out[0][0]',       \n",
      "                                                                  'conv5_block2_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block2_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block2_add[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_conv (Conv2D)   (None, 8, 8, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
      "                                                                                                  \n",
      " conv5_block3_1_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_1_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_2_conv (Conv2D)   (None, 8, 8, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_2_bn (BatchNormal  (None, 8, 8, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_2_relu (Activatio  (None, 8, 8, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " conv5_block3_3_conv (Conv2D)   (None, 8, 8, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
      "                                                                                                  \n",
      " conv5_block3_3_bn (BatchNormal  (None, 8, 8, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " conv5_block3_add (Add)         (None, 8, 8, 2048)   0           ['conv5_block2_out[0][0]',       \n",
      "                                                                  'conv5_block3_3_bn[0][0]']      \n",
      "                                                                                                  \n",
      " conv5_block3_out (Activation)  (None, 8, 8, 2048)   0           ['conv5_block3_add[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d49fa60-a5a3-406b-a89e-89e621962ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 8, 8, 2048)\n"
     ]
    }
   ],
   "source": [
    "image_batch, label_batch = next(iter(train_ds))\n",
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09f70e4d-3e83-463f-86ff-72610214d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2048)\n"
     ]
    }
   ],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d05ad483-050c-4284-9faf-1b54f6f89e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 7)\n"
     ]
    }
   ],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(7, activation='softmax')\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2a6096-8877-4792-9d90-90f6fc29a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),  # Randomly flip horizontally\n",
    "    tf.keras.layers.RandomRotation(20),        # Randomly rotate images (up to 20 degrees)\n",
    "    tf.keras.layers.RandomZoom(0.1),           # Randomly zoom images (up to 10%)\n",
    "    tf.keras.layers.RandomBrightness(0.1),     # Randomly adjust brightness (up to 10%)\n",
    "    tf.keras.layers.RandomContrast(0.1),       # Randomly adjust contrast (up to 10%)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d690885-bfa3-4593-ad46-50df8a8596a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(256, 256, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da2bbb34-c51b-47a2-923f-1ad8ae06fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 256, 256, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 256, 256, 3)      0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 8, 8, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 14343     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,602,055\n",
      "Trainable params: 14,343\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a2e85bb-960a-4bf2-9a92-54c8c02255ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd2a195-1f90-417c-b019-537e63d3c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-5\n",
    "# Create a learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=1000,  # Decay every 1000 steps/epochs\n",
    "    decay_rate=0.9,     # Decay rate\n",
    "    staircase=True)     # Decay in discrete steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b1f8726-e146-4275-9423-e9661cc3b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef1ad464-b8f9-463f-a917-1a2e631c0b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.3468 - sparse_categorical_accuracy: 0.1764\n",
      "Epoch 1: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 360s 4s/step - loss: 2.3468 - sparse_categorical_accuracy: 0.1764 - val_loss: 2.2055 - val_sparse_categorical_accuracy: 0.1629\n",
      "Epoch 2/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.2106 - sparse_categorical_accuracy: 0.1964\n",
      "Epoch 2: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 319s 4s/step - loss: 2.2106 - sparse_categorical_accuracy: 0.1964 - val_loss: 2.0723 - val_sparse_categorical_accuracy: 0.1857\n",
      "Epoch 3/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 2.0554 - sparse_categorical_accuracy: 0.2346\n",
      "Epoch 3: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 2.0554 - sparse_categorical_accuracy: 0.2346 - val_loss: 1.9816 - val_sparse_categorical_accuracy: 0.2043\n",
      "Epoch 4/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.9764 - sparse_categorical_accuracy: 0.2550\n",
      "Epoch 4: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 1.9764 - sparse_categorical_accuracy: 0.2550 - val_loss: 1.9060 - val_sparse_categorical_accuracy: 0.2514\n",
      "Epoch 5/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.9015 - sparse_categorical_accuracy: 0.2807\n",
      "Epoch 5: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 312s 4s/step - loss: 1.9015 - sparse_categorical_accuracy: 0.2807 - val_loss: 1.8391 - val_sparse_categorical_accuracy: 0.2914\n",
      "Epoch 6/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.8202 - sparse_categorical_accuracy: 0.3164\n",
      "Epoch 6: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 314s 4s/step - loss: 1.8202 - sparse_categorical_accuracy: 0.3164 - val_loss: 1.7705 - val_sparse_categorical_accuracy: 0.3157\n",
      "Epoch 7/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.7508 - sparse_categorical_accuracy: 0.3375\n",
      "Epoch 7: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 314s 4s/step - loss: 1.7508 - sparse_categorical_accuracy: 0.3375 - val_loss: 1.7097 - val_sparse_categorical_accuracy: 0.3486\n",
      "Epoch 8/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6941 - sparse_categorical_accuracy: 0.3868\n",
      "Epoch 8: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 314s 4s/step - loss: 1.6941 - sparse_categorical_accuracy: 0.3868 - val_loss: 1.6476 - val_sparse_categorical_accuracy: 0.3857\n",
      "Epoch 9/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.6179 - sparse_categorical_accuracy: 0.4093\n",
      "Epoch 9: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 1.6179 - sparse_categorical_accuracy: 0.4093 - val_loss: 1.5918 - val_sparse_categorical_accuracy: 0.4143\n",
      "Epoch 10/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.5615 - sparse_categorical_accuracy: 0.4250\n",
      "Epoch 10: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 317s 4s/step - loss: 1.5615 - sparse_categorical_accuracy: 0.4250 - val_loss: 1.5342 - val_sparse_categorical_accuracy: 0.4457\n",
      "Epoch 11/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.4927 - sparse_categorical_accuracy: 0.4671\n",
      "Epoch 11: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 317s 4s/step - loss: 1.4927 - sparse_categorical_accuracy: 0.4671 - val_loss: 1.4804 - val_sparse_categorical_accuracy: 0.4857\n",
      "Epoch 12/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.4660 - sparse_categorical_accuracy: 0.4718\n",
      "Epoch 12: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 1.4660 - sparse_categorical_accuracy: 0.4718 - val_loss: 1.4346 - val_sparse_categorical_accuracy: 0.5100\n",
      "Epoch 13/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.4359 - sparse_categorical_accuracy: 0.4904\n",
      "Epoch 13: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 1.4359 - sparse_categorical_accuracy: 0.4904 - val_loss: 1.3915 - val_sparse_categorical_accuracy: 0.5414\n",
      "Epoch 14/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.3610 - sparse_categorical_accuracy: 0.5264\n",
      "Epoch 14: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 1.3610 - sparse_categorical_accuracy: 0.5264 - val_loss: 1.3518 - val_sparse_categorical_accuracy: 0.5657\n",
      "Epoch 15/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.3168 - sparse_categorical_accuracy: 0.5368\n",
      "Epoch 15: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 1.3168 - sparse_categorical_accuracy: 0.5368 - val_loss: 1.3124 - val_sparse_categorical_accuracy: 0.5829\n",
      "Epoch 16/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.2806 - sparse_categorical_accuracy: 0.5657\n",
      "Epoch 16: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 1.2806 - sparse_categorical_accuracy: 0.5657 - val_loss: 1.2740 - val_sparse_categorical_accuracy: 0.6100\n",
      "Epoch 17/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.2416 - sparse_categorical_accuracy: 0.5782\n",
      "Epoch 17: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 3s/step - loss: 1.2416 - sparse_categorical_accuracy: 0.5782 - val_loss: 1.2394 - val_sparse_categorical_accuracy: 0.6243\n",
      "Epoch 18/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.2102 - sparse_categorical_accuracy: 0.5932\n",
      "Epoch 18: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 4s/step - loss: 1.2102 - sparse_categorical_accuracy: 0.5932 - val_loss: 1.2030 - val_sparse_categorical_accuracy: 0.6429\n",
      "Epoch 19/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1608 - sparse_categorical_accuracy: 0.6125\n",
      "Epoch 19: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 1.1608 - sparse_categorical_accuracy: 0.6125 - val_loss: 1.1693 - val_sparse_categorical_accuracy: 0.6586\n",
      "Epoch 20/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1469 - sparse_categorical_accuracy: 0.6214\n",
      "Epoch 20: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 1.1469 - sparse_categorical_accuracy: 0.6214 - val_loss: 1.1406 - val_sparse_categorical_accuracy: 0.6757\n",
      "Epoch 21/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.1117 - sparse_categorical_accuracy: 0.6304\n",
      "Epoch 21: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 1.1117 - sparse_categorical_accuracy: 0.6304 - val_loss: 1.1086 - val_sparse_categorical_accuracy: 0.6900\n",
      "Epoch 22/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0626 - sparse_categorical_accuracy: 0.6607\n",
      "Epoch 22: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 3s/step - loss: 1.0626 - sparse_categorical_accuracy: 0.6607 - val_loss: 1.0805 - val_sparse_categorical_accuracy: 0.7043\n",
      "Epoch 23/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0491 - sparse_categorical_accuracy: 0.6629\n",
      "Epoch 23: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 1.0491 - sparse_categorical_accuracy: 0.6629 - val_loss: 1.0516 - val_sparse_categorical_accuracy: 0.7129\n",
      "Epoch 24/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 1.0175 - sparse_categorical_accuracy: 0.6786\n",
      "Epoch 24: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 1.0175 - sparse_categorical_accuracy: 0.6786 - val_loss: 1.0277 - val_sparse_categorical_accuracy: 0.7200\n",
      "Epoch 25/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.9961 - sparse_categorical_accuracy: 0.6886\n",
      "Epoch 25: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 3s/step - loss: 0.9961 - sparse_categorical_accuracy: 0.6886 - val_loss: 1.0054 - val_sparse_categorical_accuracy: 0.7343\n",
      "Epoch 26/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.9919 - sparse_categorical_accuracy: 0.6882\n",
      "Epoch 26: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.9919 - sparse_categorical_accuracy: 0.6882 - val_loss: 0.9849 - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 27/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.9735 - sparse_categorical_accuracy: 0.7039\n",
      "Epoch 27: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 3s/step - loss: 0.9735 - sparse_categorical_accuracy: 0.7039 - val_loss: 0.9647 - val_sparse_categorical_accuracy: 0.7514\n",
      "Epoch 28/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.9376 - sparse_categorical_accuracy: 0.7104\n",
      "Epoch 28: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 306s 3s/step - loss: 0.9376 - sparse_categorical_accuracy: 0.7104 - val_loss: 0.9434 - val_sparse_categorical_accuracy: 0.7614\n",
      "Epoch 29/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.9039 - sparse_categorical_accuracy: 0.7321\n",
      "Epoch 29: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.9039 - sparse_categorical_accuracy: 0.7321 - val_loss: 0.9218 - val_sparse_categorical_accuracy: 0.7657\n",
      "Epoch 30/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.8971 - sparse_categorical_accuracy: 0.7282\n",
      "Epoch 30: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 315s 4s/step - loss: 0.8971 - sparse_categorical_accuracy: 0.7282 - val_loss: 0.9045 - val_sparse_categorical_accuracy: 0.7714\n",
      "Epoch 31/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.8675 - sparse_categorical_accuracy: 0.7393\n",
      "Epoch 31: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.8675 - sparse_categorical_accuracy: 0.7393 - val_loss: 0.8854 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 32/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.8572 - sparse_categorical_accuracy: 0.7418\n",
      "Epoch 32: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.8572 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.8682 - val_sparse_categorical_accuracy: 0.7843\n",
      "Epoch 33/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.8362 - sparse_categorical_accuracy: 0.7536\n",
      "Epoch 33: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.8362 - sparse_categorical_accuracy: 0.7536 - val_loss: 0.8492 - val_sparse_categorical_accuracy: 0.7857\n",
      "Epoch 34/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.8131 - sparse_categorical_accuracy: 0.7732\n",
      "Epoch 34: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.8131 - sparse_categorical_accuracy: 0.7732 - val_loss: 0.8331 - val_sparse_categorical_accuracy: 0.7957\n",
      "Epoch 35/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7971 - sparse_categorical_accuracy: 0.7718\n",
      "Epoch 35: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.7971 - sparse_categorical_accuracy: 0.7718 - val_loss: 0.8174 - val_sparse_categorical_accuracy: 0.8043\n",
      "Epoch 36/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7925 - sparse_categorical_accuracy: 0.7718\n",
      "Epoch 36: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 3s/step - loss: 0.7925 - sparse_categorical_accuracy: 0.7718 - val_loss: 0.8064 - val_sparse_categorical_accuracy: 0.8143\n",
      "Epoch 37/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7726 - sparse_categorical_accuracy: 0.7789\n",
      "Epoch 37: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 3s/step - loss: 0.7726 - sparse_categorical_accuracy: 0.7789 - val_loss: 0.7914 - val_sparse_categorical_accuracy: 0.8171\n",
      "Epoch 38/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7666 - sparse_categorical_accuracy: 0.7825\n",
      "Epoch 38: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 316s 4s/step - loss: 0.7666 - sparse_categorical_accuracy: 0.7825 - val_loss: 0.7787 - val_sparse_categorical_accuracy: 0.8214\n",
      "Epoch 39/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7431 - sparse_categorical_accuracy: 0.7968\n",
      "Epoch 39: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 315s 4s/step - loss: 0.7431 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.7656 - val_sparse_categorical_accuracy: 0.8229\n",
      "Epoch 40/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7359 - sparse_categorical_accuracy: 0.7936\n",
      "Epoch 40: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.7359 - sparse_categorical_accuracy: 0.7936 - val_loss: 0.7538 - val_sparse_categorical_accuracy: 0.8314\n",
      "Epoch 41/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7068 - sparse_categorical_accuracy: 0.8025\n",
      "Epoch 41: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.7068 - sparse_categorical_accuracy: 0.8025 - val_loss: 0.7423 - val_sparse_categorical_accuracy: 0.8300\n",
      "Epoch 42/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.7125 - sparse_categorical_accuracy: 0.8075\n",
      "Epoch 42: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 329s 4s/step - loss: 0.7125 - sparse_categorical_accuracy: 0.8075 - val_loss: 0.7308 - val_sparse_categorical_accuracy: 0.8314\n",
      "Epoch 43/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6924 - sparse_categorical_accuracy: 0.8036\n",
      "Epoch 43: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 365s 4s/step - loss: 0.6924 - sparse_categorical_accuracy: 0.8036 - val_loss: 0.7193 - val_sparse_categorical_accuracy: 0.8343\n",
      "Epoch 44/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6859 - sparse_categorical_accuracy: 0.8121\n",
      "Epoch 44: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 381s 4s/step - loss: 0.6859 - sparse_categorical_accuracy: 0.8121 - val_loss: 0.7072 - val_sparse_categorical_accuracy: 0.8400\n",
      "Epoch 45/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6759 - sparse_categorical_accuracy: 0.8154\n",
      "Epoch 45: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 348s 4s/step - loss: 0.6759 - sparse_categorical_accuracy: 0.8154 - val_loss: 0.6969 - val_sparse_categorical_accuracy: 0.8443\n",
      "Epoch 46/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6682 - sparse_categorical_accuracy: 0.8182\n",
      "Epoch 46: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 316s 4s/step - loss: 0.6682 - sparse_categorical_accuracy: 0.8182 - val_loss: 0.6864 - val_sparse_categorical_accuracy: 0.8471\n",
      "Epoch 47/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6527 - sparse_categorical_accuracy: 0.8189\n",
      "Epoch 47: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.6527 - sparse_categorical_accuracy: 0.8189 - val_loss: 0.6777 - val_sparse_categorical_accuracy: 0.8514\n",
      "Epoch 48/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6472 - sparse_categorical_accuracy: 0.8214\n",
      "Epoch 48: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.6472 - sparse_categorical_accuracy: 0.8214 - val_loss: 0.6684 - val_sparse_categorical_accuracy: 0.8543\n",
      "Epoch 49/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6489 - sparse_categorical_accuracy: 0.8307\n",
      "Epoch 49: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.6489 - sparse_categorical_accuracy: 0.8307 - val_loss: 0.6595 - val_sparse_categorical_accuracy: 0.8557\n",
      "Epoch 50/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6296 - sparse_categorical_accuracy: 0.8361\n",
      "Epoch 50: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 4s/step - loss: 0.6296 - sparse_categorical_accuracy: 0.8361 - val_loss: 0.6510 - val_sparse_categorical_accuracy: 0.8557\n",
      "Epoch 51/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6270 - sparse_categorical_accuracy: 0.8364\n",
      "Epoch 51: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.6270 - sparse_categorical_accuracy: 0.8364 - val_loss: 0.6417 - val_sparse_categorical_accuracy: 0.8586\n",
      "Epoch 52/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6107 - sparse_categorical_accuracy: 0.8393\n",
      "Epoch 52: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 313s 4s/step - loss: 0.6107 - sparse_categorical_accuracy: 0.8393 - val_loss: 0.6331 - val_sparse_categorical_accuracy: 0.8600\n",
      "Epoch 53/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6117 - sparse_categorical_accuracy: 0.8350\n",
      "Epoch 53: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.6117 - sparse_categorical_accuracy: 0.8350 - val_loss: 0.6247 - val_sparse_categorical_accuracy: 0.8614\n",
      "Epoch 54/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.6109 - sparse_categorical_accuracy: 0.8425\n",
      "Epoch 54: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.6109 - sparse_categorical_accuracy: 0.8425 - val_loss: 0.6176 - val_sparse_categorical_accuracy: 0.8614\n",
      "Epoch 55/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5946 - sparse_categorical_accuracy: 0.8361\n",
      "Epoch 55: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.5946 - sparse_categorical_accuracy: 0.8361 - val_loss: 0.6095 - val_sparse_categorical_accuracy: 0.8671\n",
      "Epoch 56/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5674 - sparse_categorical_accuracy: 0.8518\n",
      "Epoch 56: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.5674 - sparse_categorical_accuracy: 0.8518 - val_loss: 0.6054 - val_sparse_categorical_accuracy: 0.8671\n",
      "Epoch 57/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5848 - sparse_categorical_accuracy: 0.8454\n",
      "Epoch 57: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.5848 - sparse_categorical_accuracy: 0.8454 - val_loss: 0.5968 - val_sparse_categorical_accuracy: 0.8686\n",
      "Epoch 58/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5740 - sparse_categorical_accuracy: 0.8496\n",
      "Epoch 58: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.5740 - sparse_categorical_accuracy: 0.8496 - val_loss: 0.5904 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 59/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5646 - sparse_categorical_accuracy: 0.8493\n",
      "Epoch 59: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 313s 4s/step - loss: 0.5646 - sparse_categorical_accuracy: 0.8493 - val_loss: 0.5846 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 60/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5616 - sparse_categorical_accuracy: 0.8511\n",
      "Epoch 60: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.5616 - sparse_categorical_accuracy: 0.8511 - val_loss: 0.5793 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 61/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5455 - sparse_categorical_accuracy: 0.8543\n",
      "Epoch 61: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.5455 - sparse_categorical_accuracy: 0.8543 - val_loss: 0.5736 - val_sparse_categorical_accuracy: 0.8714\n",
      "Epoch 62/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5406 - sparse_categorical_accuracy: 0.8625\n",
      "Epoch 62: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 4s/step - loss: 0.5406 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.5678 - val_sparse_categorical_accuracy: 0.8743\n",
      "Epoch 63/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5242 - sparse_categorical_accuracy: 0.8771\n",
      "Epoch 63: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.5242 - sparse_categorical_accuracy: 0.8771 - val_loss: 0.5622 - val_sparse_categorical_accuracy: 0.8714\n",
      "Epoch 64/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5235 - sparse_categorical_accuracy: 0.8671\n",
      "Epoch 64: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 314s 4s/step - loss: 0.5235 - sparse_categorical_accuracy: 0.8671 - val_loss: 0.5565 - val_sparse_categorical_accuracy: 0.8743\n",
      "Epoch 65/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5309 - sparse_categorical_accuracy: 0.8668\n",
      "Epoch 65: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 312s 4s/step - loss: 0.5309 - sparse_categorical_accuracy: 0.8668 - val_loss: 0.5507 - val_sparse_categorical_accuracy: 0.8771\n",
      "Epoch 66/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5097 - sparse_categorical_accuracy: 0.8725\n",
      "Epoch 66: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.5097 - sparse_categorical_accuracy: 0.8725 - val_loss: 0.5445 - val_sparse_categorical_accuracy: 0.8786\n",
      "Epoch 67/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5275 - sparse_categorical_accuracy: 0.8629\n",
      "Epoch 67: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.5275 - sparse_categorical_accuracy: 0.8629 - val_loss: 0.5394 - val_sparse_categorical_accuracy: 0.8800\n",
      "Epoch 68/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5031 - sparse_categorical_accuracy: 0.8739\n",
      "Epoch 68: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 3s/step - loss: 0.5031 - sparse_categorical_accuracy: 0.8739 - val_loss: 0.5331 - val_sparse_categorical_accuracy: 0.8829\n",
      "Epoch 69/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5023 - sparse_categorical_accuracy: 0.8757\n",
      "Epoch 69: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.5023 - sparse_categorical_accuracy: 0.8757 - val_loss: 0.5293 - val_sparse_categorical_accuracy: 0.8829\n",
      "Epoch 70/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4977 - sparse_categorical_accuracy: 0.8775\n",
      "Epoch 70: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4977 - sparse_categorical_accuracy: 0.8775 - val_loss: 0.5245 - val_sparse_categorical_accuracy: 0.8857\n",
      "Epoch 71/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.5004 - sparse_categorical_accuracy: 0.8736\n",
      "Epoch 71: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.5004 - sparse_categorical_accuracy: 0.8736 - val_loss: 0.5211 - val_sparse_categorical_accuracy: 0.8857\n",
      "Epoch 72/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4873 - sparse_categorical_accuracy: 0.8729\n",
      "Epoch 72: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4873 - sparse_categorical_accuracy: 0.8729 - val_loss: 0.5165 - val_sparse_categorical_accuracy: 0.8843\n",
      "Epoch 73/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4862 - sparse_categorical_accuracy: 0.8811\n",
      "Epoch 73: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4862 - sparse_categorical_accuracy: 0.8811 - val_loss: 0.5123 - val_sparse_categorical_accuracy: 0.8871\n",
      "Epoch 74/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4728 - sparse_categorical_accuracy: 0.8832\n",
      "Epoch 74: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.4728 - sparse_categorical_accuracy: 0.8832 - val_loss: 0.5065 - val_sparse_categorical_accuracy: 0.8886\n",
      "Epoch 75/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4793 - sparse_categorical_accuracy: 0.8836\n",
      "Epoch 75: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 316s 4s/step - loss: 0.4793 - sparse_categorical_accuracy: 0.8836 - val_loss: 0.5013 - val_sparse_categorical_accuracy: 0.8914\n",
      "Epoch 76/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4749 - sparse_categorical_accuracy: 0.8804\n",
      "Epoch 76: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 314s 4s/step - loss: 0.4749 - sparse_categorical_accuracy: 0.8804 - val_loss: 0.4995 - val_sparse_categorical_accuracy: 0.8943\n",
      "Epoch 77/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4738 - sparse_categorical_accuracy: 0.8807\n",
      "Epoch 77: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 312s 4s/step - loss: 0.4738 - sparse_categorical_accuracy: 0.8807 - val_loss: 0.4954 - val_sparse_categorical_accuracy: 0.8914\n",
      "Epoch 78/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4653 - sparse_categorical_accuracy: 0.8843\n",
      "Epoch 78: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4653 - sparse_categorical_accuracy: 0.8843 - val_loss: 0.4902 - val_sparse_categorical_accuracy: 0.8943\n",
      "Epoch 79/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4610 - sparse_categorical_accuracy: 0.8793\n",
      "Epoch 79: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.4610 - sparse_categorical_accuracy: 0.8793 - val_loss: 0.4853 - val_sparse_categorical_accuracy: 0.8971\n",
      "Epoch 80/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4497 - sparse_categorical_accuracy: 0.8886\n",
      "Epoch 80: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4497 - sparse_categorical_accuracy: 0.8886 - val_loss: 0.4826 - val_sparse_categorical_accuracy: 0.8986\n",
      "Epoch 81/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4543 - sparse_categorical_accuracy: 0.8857\n",
      "Epoch 81: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.4543 - sparse_categorical_accuracy: 0.8857 - val_loss: 0.4788 - val_sparse_categorical_accuracy: 0.8957\n",
      "Epoch 82/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4516 - sparse_categorical_accuracy: 0.8879\n",
      "Epoch 82: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.4516 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.4752 - val_sparse_categorical_accuracy: 0.8986\n",
      "Epoch 83/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4421 - sparse_categorical_accuracy: 0.8882\n",
      "Epoch 83: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.4421 - sparse_categorical_accuracy: 0.8882 - val_loss: 0.4723 - val_sparse_categorical_accuracy: 0.8986\n",
      "Epoch 84/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4414 - sparse_categorical_accuracy: 0.8871\n",
      "Epoch 84: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 313s 4s/step - loss: 0.4414 - sparse_categorical_accuracy: 0.8871 - val_loss: 0.4690 - val_sparse_categorical_accuracy: 0.8986\n",
      "Epoch 85/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4524 - sparse_categorical_accuracy: 0.8821\n",
      "Epoch 85: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4524 - sparse_categorical_accuracy: 0.8821 - val_loss: 0.4660 - val_sparse_categorical_accuracy: 0.9043\n",
      "Epoch 86/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4338 - sparse_categorical_accuracy: 0.8836\n",
      "Epoch 86: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4338 - sparse_categorical_accuracy: 0.8836 - val_loss: 0.4626 - val_sparse_categorical_accuracy: 0.9029\n",
      "Epoch 87/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4352 - sparse_categorical_accuracy: 0.8882\n",
      "Epoch 87: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 315s 4s/step - loss: 0.4352 - sparse_categorical_accuracy: 0.8882 - val_loss: 0.4594 - val_sparse_categorical_accuracy: 0.9029\n",
      "Epoch 88/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4287 - sparse_categorical_accuracy: 0.8943\n",
      "Epoch 88: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.4287 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.4562 - val_sparse_categorical_accuracy: 0.9057\n",
      "Epoch 89/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4264 - sparse_categorical_accuracy: 0.8950\n",
      "Epoch 89: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.4264 - sparse_categorical_accuracy: 0.8950 - val_loss: 0.4523 - val_sparse_categorical_accuracy: 0.9057\n",
      "Epoch 90/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4335 - sparse_categorical_accuracy: 0.8900\n",
      "Epoch 90: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.4335 - sparse_categorical_accuracy: 0.8900 - val_loss: 0.4500 - val_sparse_categorical_accuracy: 0.9057\n",
      "Epoch 91/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4163 - sparse_categorical_accuracy: 0.9029\n",
      "Epoch 91: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.4163 - sparse_categorical_accuracy: 0.9029 - val_loss: 0.4476 - val_sparse_categorical_accuracy: 0.9043\n",
      "Epoch 92/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4233 - sparse_categorical_accuracy: 0.8957\n",
      "Epoch 92: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.4233 - sparse_categorical_accuracy: 0.8957 - val_loss: 0.4443 - val_sparse_categorical_accuracy: 0.9071\n",
      "Epoch 93/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4116 - sparse_categorical_accuracy: 0.9004\n",
      "Epoch 93: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.4116 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.4422 - val_sparse_categorical_accuracy: 0.9086\n",
      "Epoch 94/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4201 - sparse_categorical_accuracy: 0.9004\n",
      "Epoch 94: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.4201 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.4394 - val_sparse_categorical_accuracy: 0.9086\n",
      "Epoch 95/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4048 - sparse_categorical_accuracy: 0.8989\n",
      "Epoch 95: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 307s 4s/step - loss: 0.4048 - sparse_categorical_accuracy: 0.8989 - val_loss: 0.4362 - val_sparse_categorical_accuracy: 0.9114\n",
      "Epoch 96/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4104 - sparse_categorical_accuracy: 0.8996\n",
      "Epoch 96: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.4104 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.4336 - val_sparse_categorical_accuracy: 0.9114\n",
      "Epoch 97/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4029 - sparse_categorical_accuracy: 0.9004\n",
      "Epoch 97: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.4029 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.4326 - val_sparse_categorical_accuracy: 0.9114\n",
      "Epoch 98/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4030 - sparse_categorical_accuracy: 0.8993\n",
      "Epoch 98: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 313s 4s/step - loss: 0.4030 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.4292 - val_sparse_categorical_accuracy: 0.9143\n",
      "Epoch 99/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3911 - sparse_categorical_accuracy: 0.9068\n",
      "Epoch 99: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3911 - sparse_categorical_accuracy: 0.9068 - val_loss: 0.4262 - val_sparse_categorical_accuracy: 0.9143\n",
      "Epoch 100/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3945 - sparse_categorical_accuracy: 0.8989\n",
      "Epoch 100: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 317s 4s/step - loss: 0.3945 - sparse_categorical_accuracy: 0.8989 - val_loss: 0.4223 - val_sparse_categorical_accuracy: 0.9143\n",
      "Epoch 101/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3898 - sparse_categorical_accuracy: 0.9096\n",
      "Epoch 101: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 316s 4s/step - loss: 0.3898 - sparse_categorical_accuracy: 0.9096 - val_loss: 0.4205 - val_sparse_categorical_accuracy: 0.9143\n",
      "Epoch 102/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.4007 - sparse_categorical_accuracy: 0.9011\n",
      "Epoch 102: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.4007 - sparse_categorical_accuracy: 0.9011 - val_loss: 0.4185 - val_sparse_categorical_accuracy: 0.9143\n",
      "Epoch 103/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3912 - sparse_categorical_accuracy: 0.9057\n",
      "Epoch 103: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3912 - sparse_categorical_accuracy: 0.9057 - val_loss: 0.4157 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 104/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3945 - sparse_categorical_accuracy: 0.9075\n",
      "Epoch 104: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.3945 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.4140 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 105/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3951 - sparse_categorical_accuracy: 0.8989\n",
      "Epoch 105: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3951 - sparse_categorical_accuracy: 0.8989 - val_loss: 0.4123 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 106/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3911 - sparse_categorical_accuracy: 0.9054\n",
      "Epoch 106: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3911 - sparse_categorical_accuracy: 0.9054 - val_loss: 0.4101 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 107/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3878 - sparse_categorical_accuracy: 0.9025\n",
      "Epoch 107: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3878 - sparse_categorical_accuracy: 0.9025 - val_loss: 0.4072 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 108/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3903 - sparse_categorical_accuracy: 0.8979\n",
      "Epoch 108: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3903 - sparse_categorical_accuracy: 0.8979 - val_loss: 0.4055 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 109/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3838 - sparse_categorical_accuracy: 0.9100\n",
      "Epoch 109: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.3838 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.4039 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 110/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3835 - sparse_categorical_accuracy: 0.8996\n",
      "Epoch 110: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3835 - sparse_categorical_accuracy: 0.8996 - val_loss: 0.4017 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 111/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3659 - sparse_categorical_accuracy: 0.9143\n",
      "Epoch 111: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3659 - sparse_categorical_accuracy: 0.9143 - val_loss: 0.4001 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 112/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3797 - sparse_categorical_accuracy: 0.9082\n",
      "Epoch 112: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3797 - sparse_categorical_accuracy: 0.9082 - val_loss: 0.3985 - val_sparse_categorical_accuracy: 0.9157\n",
      "Epoch 113/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3711 - sparse_categorical_accuracy: 0.9107\n",
      "Epoch 113: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3711 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.3969 - val_sparse_categorical_accuracy: 0.9186\n",
      "Epoch 114/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3704 - sparse_categorical_accuracy: 0.9068\n",
      "Epoch 114: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3704 - sparse_categorical_accuracy: 0.9068 - val_loss: 0.3957 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 115/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3688 - sparse_categorical_accuracy: 0.9118\n",
      "Epoch 115: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3688 - sparse_categorical_accuracy: 0.9118 - val_loss: 0.3940 - val_sparse_categorical_accuracy: 0.9229\n",
      "Epoch 116/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3627 - sparse_categorical_accuracy: 0.9143\n",
      "Epoch 116: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 315s 4s/step - loss: 0.3627 - sparse_categorical_accuracy: 0.9143 - val_loss: 0.3919 - val_sparse_categorical_accuracy: 0.9214\n",
      "Epoch 117/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3572 - sparse_categorical_accuracy: 0.9164\n",
      "Epoch 117: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 313s 4s/step - loss: 0.3572 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.3907 - val_sparse_categorical_accuracy: 0.9229\n",
      "Epoch 118/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3512 - sparse_categorical_accuracy: 0.9150\n",
      "Epoch 118: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3512 - sparse_categorical_accuracy: 0.9150 - val_loss: 0.3895 - val_sparse_categorical_accuracy: 0.9214\n",
      "Epoch 119/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3658 - sparse_categorical_accuracy: 0.9075\n",
      "Epoch 119: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3658 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.3872 - val_sparse_categorical_accuracy: 0.9229\n",
      "Epoch 120/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3774 - sparse_categorical_accuracy: 0.9043\n",
      "Epoch 120: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3774 - sparse_categorical_accuracy: 0.9043 - val_loss: 0.3863 - val_sparse_categorical_accuracy: 0.9229\n",
      "Epoch 121/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3630 - sparse_categorical_accuracy: 0.9068\n",
      "Epoch 121: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3630 - sparse_categorical_accuracy: 0.9068 - val_loss: 0.3848 - val_sparse_categorical_accuracy: 0.9200\n",
      "Epoch 122/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3571 - sparse_categorical_accuracy: 0.9139\n",
      "Epoch 122: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 317s 4s/step - loss: 0.3571 - sparse_categorical_accuracy: 0.9139 - val_loss: 0.3827 - val_sparse_categorical_accuracy: 0.9229\n",
      "Epoch 123/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3653 - sparse_categorical_accuracy: 0.9079\n",
      "Epoch 123: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3653 - sparse_categorical_accuracy: 0.9079 - val_loss: 0.3805 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 124/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3674 - sparse_categorical_accuracy: 0.9089\n",
      "Epoch 124: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3674 - sparse_categorical_accuracy: 0.9089 - val_loss: 0.3786 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 125/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3548 - sparse_categorical_accuracy: 0.9114\n",
      "Epoch 125: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3548 - sparse_categorical_accuracy: 0.9114 - val_loss: 0.3774 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 126/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3453 - sparse_categorical_accuracy: 0.9161\n",
      "Epoch 126: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3453 - sparse_categorical_accuracy: 0.9161 - val_loss: 0.3765 - val_sparse_categorical_accuracy: 0.9214\n",
      "Epoch 127/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3484 - sparse_categorical_accuracy: 0.9189\n",
      "Epoch 127: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3484 - sparse_categorical_accuracy: 0.9189 - val_loss: 0.3756 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 128/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3522 - sparse_categorical_accuracy: 0.9121\n",
      "Epoch 128: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.3522 - sparse_categorical_accuracy: 0.9121 - val_loss: 0.3738 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 129/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3496 - sparse_categorical_accuracy: 0.9129\n",
      "Epoch 129: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3496 - sparse_categorical_accuracy: 0.9129 - val_loss: 0.3726 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 130/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3453 - sparse_categorical_accuracy: 0.9171\n",
      "Epoch 130: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 312s 4s/step - loss: 0.3453 - sparse_categorical_accuracy: 0.9171 - val_loss: 0.3712 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 131/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3426 - sparse_categorical_accuracy: 0.9154\n",
      "Epoch 131: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 312s 4s/step - loss: 0.3426 - sparse_categorical_accuracy: 0.9154 - val_loss: 0.3695 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 132/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3407 - sparse_categorical_accuracy: 0.9154\n",
      "Epoch 132: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 325s 4s/step - loss: 0.3407 - sparse_categorical_accuracy: 0.9154 - val_loss: 0.3672 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 133/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3418 - sparse_categorical_accuracy: 0.9175\n",
      "Epoch 133: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 317s 4s/step - loss: 0.3418 - sparse_categorical_accuracy: 0.9175 - val_loss: 0.3666 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 134/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3413 - sparse_categorical_accuracy: 0.9143\n",
      "Epoch 134: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3413 - sparse_categorical_accuracy: 0.9143 - val_loss: 0.3652 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 135/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3522 - sparse_categorical_accuracy: 0.9136\n",
      "Epoch 135: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3522 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 136/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3411 - sparse_categorical_accuracy: 0.9179\n",
      "Epoch 136: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3411 - sparse_categorical_accuracy: 0.9179 - val_loss: 0.3628 - val_sparse_categorical_accuracy: 0.9257\n",
      "Epoch 137/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3300 - sparse_categorical_accuracy: 0.9229\n",
      "Epoch 137: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3300 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.3622 - val_sparse_categorical_accuracy: 0.9257\n",
      "Epoch 138/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3370 - sparse_categorical_accuracy: 0.9150\n",
      "Epoch 138: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3370 - sparse_categorical_accuracy: 0.9150 - val_loss: 0.3611 - val_sparse_categorical_accuracy: 0.9257\n",
      "Epoch 139/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3289 - sparse_categorical_accuracy: 0.9246\n",
      "Epoch 139: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 311s 4s/step - loss: 0.3289 - sparse_categorical_accuracy: 0.9246 - val_loss: 0.3603 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 140/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3391 - sparse_categorical_accuracy: 0.9196\n",
      "Epoch 140: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.3391 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.3592 - val_sparse_categorical_accuracy: 0.9243\n",
      "Epoch 141/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3307 - sparse_categorical_accuracy: 0.9204\n",
      "Epoch 141: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3307 - sparse_categorical_accuracy: 0.9204 - val_loss: 0.3581 - val_sparse_categorical_accuracy: 0.9257\n",
      "Epoch 142/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3441 - sparse_categorical_accuracy: 0.9100\n",
      "Epoch 142: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3441 - sparse_categorical_accuracy: 0.9100 - val_loss: 0.3568 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 143/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3347 - sparse_categorical_accuracy: 0.9171\n",
      "Epoch 143: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3347 - sparse_categorical_accuracy: 0.9171 - val_loss: 0.3555 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 144/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3334 - sparse_categorical_accuracy: 0.9196\n",
      "Epoch 144: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 308s 4s/step - loss: 0.3334 - sparse_categorical_accuracy: 0.9196 - val_loss: 0.3541 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 145/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3338 - sparse_categorical_accuracy: 0.9182\n",
      "Epoch 145: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3338 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.3533 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 146/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3232 - sparse_categorical_accuracy: 0.9182\n",
      "Epoch 146: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3232 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.3522 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 147/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3310 - sparse_categorical_accuracy: 0.9189\n",
      "Epoch 147: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 309s 4s/step - loss: 0.3310 - sparse_categorical_accuracy: 0.9189 - val_loss: 0.3508 - val_sparse_categorical_accuracy: 0.9286\n",
      "Epoch 148/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3233 - sparse_categorical_accuracy: 0.9193\n",
      "Epoch 148: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 310s 4s/step - loss: 0.3233 - sparse_categorical_accuracy: 0.9193 - val_loss: 0.3502 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 149/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3274 - sparse_categorical_accuracy: 0.9182\n",
      "Epoch 149: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 314s 4s/step - loss: 0.3274 - sparse_categorical_accuracy: 0.9182 - val_loss: 0.3493 - val_sparse_categorical_accuracy: 0.9271\n",
      "Epoch 150/150\n",
      "88/88 [==============================] - ETA: 0s - loss: 0.3312 - sparse_categorical_accuracy: 0.9164\n",
      "Epoch 150: saving model to training_1\\cp.ckpt\n",
      "88/88 [==============================] - 314s 4s/step - loss: 0.3312 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.3482 - val_sparse_categorical_accuracy: 0.9271\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "history = model.fit(train_ds,\n",
    "                    epochs=150,\n",
    "                    callbacks=[EarlyStopping(patience=3, restore_best_weights=True),cp_callback],\n",
    "                    validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3964faad-6447-423f-a91c-539c8b923703",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ResNet.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66120909-d74f-4d5d-9e2d-9ab83ab0c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ResNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75c97434-b5d5-4140-88e6-1b954af8b23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehul\\AppData\\Local\\Temp\\tmpt9asicc1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\nehul\\AppData\\Local\\Temp\\tmpt9asicc1\\assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('NewModel.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3a60fb1-1667-4541-add9-ff57396172b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model('ResNet.keras')  # Adjust the path to where your model is stored\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\" Load and preprocess an image from the file path \"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize((256, 256))  # Resize image to match model's expected input shape\n",
    "    img_array = img_to_array(img)  # Convert the image to a numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add a batch dimension\n",
    "    img_array = preprocess_input(img_array)  # Preprocess the image\n",
    "    return img, img_array\n",
    "\n",
    "def predict_image(model, img_array):\n",
    "    \"\"\" Predict the class of an image \"\"\"\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    confidence = np.max(predictions)\n",
    "    return class_names[predicted_class], confidence\n",
    "\n",
    "def display_image_with_prediction(img, predicted_class, confidence):\n",
    "    \"\"\" Display image with predicted class and confidence \"\"\"\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.load_default()  # Load default font\n",
    "    text = f'{predicted_class} ({confidence*100:.2f}%)'\n",
    "    draw.text((10, 10), text, font=font, fill='red')  # Draw text on top left corner\n",
    "    img.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"C:\\Users\\nehul\\OneDrive\\Desktop\\Project\\Dataset v2\\100 Rupees Note\\0_100IndianRupees_5_jpeg.rf.c0622783b850b3766513ff1532a70459.jpg\" # Replace with your image path\n",
    "    img, img_processed = load_and_preprocess_image(image_path)\n",
    "    predicted_class, confidence = predict_image(model, img_processed)\n",
    "    display_image_with_prediction(img, predicted_class, confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf1d09e5-fb64-4906-8fe7-3e4bf5eaf456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"Model_v2.keras\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # '0' is typically the built-in webcam\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "    \n",
    "    # Resize frame to match model's expected input size, e.g., 224x224\n",
    "    input_frame = cv2.resize(frame, (256, 256))\n",
    "    \n",
    "    # Convert the frame to a format suitable for the model\n",
    "    input_frame = np.expand_dims(input_frame, axis=0)\n",
    "    input_frame = tf.keras.applications.resnet50.preprocess_input(input_frame)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(input_frame)\n",
    "    predicted_class = np.argmax(predictions)\n",
    "    confidence = np.max(predictions) * 100  # Confidence percentage\n",
    "\n",
    "    # Display the resulting frame with the prediction\n",
    "    label = f\"Class: {predicted_class}, Confidence: {confidence:.2f}%\"\n",
    "    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Break the loop with 'q'\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3057c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class_names = ['10', '100', '20', '200', '2000', '50', '500']\n",
    "\n",
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path='NewModel.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Configure camera settings\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera could not be opened.\")\n",
    "    exit()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, size):\n",
    "    frame_resized = cv2.resize(frame, size)  # Adjust size to match model's expected input\n",
    "    frame_preprocessed = tf.keras.applications.resnet.preprocess_input(frame_resized)\n",
    "    return frame_preprocessed\n",
    "\n",
    "# Prediction loop\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        # Preprocess the image to fit the model input\n",
    "        processed_frame = preprocess_frame(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n",
    "        processed_frame = np.expand_dims(processed_frame, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], processed_frame.astype(np.float32))\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Extract the output and display the prediction\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions = tf.nn.softmax(output_data[0])  # Apply softmax to logits\n",
    "        predicted_class = class_names[np.argmax(predictions)]\n",
    "        confidence = np.max(predictions) * 100  # Max probability as confidence\n",
    "\n",
    "        # Display predicted class and confidence on the frame\n",
    "        if confidence < 19.5:\n",
    "            label = \"No currency detected\"\n",
    "        else:\n",
    "            label = f\"Note: {predicted_class} Rupees   Confidence: {confidence:.2f}%\"\n",
    "        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "finally:\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "\n",
    "class_names = ['10', '100', '20', '200', '2000', '50', '500']\n",
    "\n",
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path='NewModel.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Configure camera settings\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera could not be opened.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, size):\n",
    "    frame_resized = cv2.resize(frame, size)  # Adjust size to match model's expected input\n",
    "    frame_preprocessed = tf.keras.applications.resnet.preprocess_input(frame_resized)\n",
    "    return frame_preprocessed\n",
    "\n",
    "# Prediction loop\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        # Preprocess the image to fit the model input\n",
    "        processed_frame = preprocess_frame(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n",
    "        processed_frame = np.expand_dims(processed_frame, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], processed_frame.astype(np.float32))\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Extract the output and display the prediction\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions = tf.nn.softmax(output_data[0])  # Apply softmax to logits\n",
    "        predicted_class = class_names[np.argmax(predictions)]\n",
    "        confidence = np.max(predictions) * 100  # Max probability as confidence\n",
    "\n",
    "        # Display predicted class and confidence on the frame\n",
    "        if confidence < 20:\n",
    "            label = \"No currency detected\"\n",
    "        else:\n",
    "            label = f\"Note: {predicted_class} Rupees   Confidence: {confidence:.2f}%\"\n",
    "\n",
    "        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Announce the prediction when spacebar is pressed\n",
    "        if cv2.waitKey(1) == 32:  # ASCII value for spacebar is 32\n",
    "            if confidence >= 20:\n",
    "                engine.say(f\"{predicted_class} Rupees\")\n",
    "                engine.runAndWait()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted by user\")\n",
    "\n",
    "finally:\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    engine.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d271829a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mset_tensor(input_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], processed_frame\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Extract the output and display the prediction\u001b[39;00m\n\u001b[0;32m     50\u001b[0m output_data \u001b[38;5;241m=\u001b[39m interpreter\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\nehul\\OneDrive\\Desktop\\Project\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:941\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> 941\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "\n",
    "class_names = ['10', '100', '20', '200', '2000', '50', '500']\n",
    "\n",
    "# Load TFLite model and allocate tensors\n",
    "interpreter = tf.lite.Interpreter(model_path='NewModel.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Configure camera settings\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Camera could not be opened.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Function to preprocess the frame\n",
    "def preprocess_frame(frame, size):\n",
    "    frame_resized = cv2.resize(frame, size)  # Adjust size to match model's expected input\n",
    "    frame_preprocessed = tf.keras.applications.resnet.preprocess_input(frame_resized)\n",
    "    return frame_preprocessed\n",
    "\n",
    "# Prediction loop\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "\n",
    "        # Preprocess the image to fit the model input\n",
    "        processed_frame = preprocess_frame(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n",
    "        processed_frame = np.expand_dims(processed_frame, axis=0)  # Add batch dimension\n",
    "\n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], processed_frame.astype(np.float32))\n",
    "\n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Extract the output and display the prediction\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions = tf.nn.softmax(output_data[0])  # Apply softmax to logits\n",
    "        predicted_class = class_names[np.argmax(predictions)]\n",
    "        confidence = np.max(predictions) * 100  # Max probability as confidence\n",
    "\n",
    "        # Display predicted class and confidence on the frame\n",
    "        if confidence < 19.5:\n",
    "            label = \"No currency detected\"\n",
    "        else:\n",
    "            label = f\"Note: {predicted_class} Rupees   Confidence: {confidence:.2f}%\"\n",
    "\n",
    "        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Announce the prediction when spacebar is pressedq\n",
    "        if cv2.waitKey(1) == 32:  # ASCII value for spacebar is 32\n",
    "            if confidence >= 19.5:\n",
    "                engine.say(f\"{predicted_class} Rupees\")\n",
    "            else:\n",
    "                engine.say(\"No currency detected\")    \n",
    "            engine.runAndWait()\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "finally:\n",
    "    # When everything done, release the capture\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    engine.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
