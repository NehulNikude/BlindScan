# -*- coding: utf-8 -*-
"""BlindScan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t9yolJuhlzHdtztjZdJ24u2StN8DFfut
"""

!pip install roboflow

#this is sample to binary classify the 20 Rupee Note
from roboflow import Roboflow
rf = Roboflow(api_key="357NsZXmucflaxgKU9Gj")
project = rf.workspace("segunda-revision").project("notedetection-7foeq")
version = project.version(1)
dataset = version.download("tfrecord")

import tensorflow as tf
from tensorflow import keras
import os, warnings
import matplotlib.pyplot as plt
from matplotlib import gridspec
import numpy as np
#from tensorflow.keras.preprocessing import image_dataset_from_directory

def parse_tfrecord(tfrecord):
    feature_description = {
        'image/encoded': tf.io.FixedLenFeature([], tf.string),  # Image as a string
        'image/class/label': tf.io.FixedLenFeature([], tf.int64),  # Label
        # Include additional features if present
    }
    example = tf.io.parse_single_example(tfrecord, feature_description)
    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)  # Adjust channels according to your images
    image = tf.image.resize(image, [224, 224])  # Resize image if needed
    label = example['image/class/label']
    return image, label

def load_dataset(tfrecord_files, batch_size):
    raw_dataset = tf.data.TFRecordDataset(tfrecord_files)
    parsed_dataset = raw_dataset.map(parse_tfrecord)
    return parsed_dataset.batch(batch_size)

# Example usage
train_files = ["/content/NoteDetection-4/train/notes.tfrecord"]  # Update this path
batch_size = 32
train_dataset = load_dataset(train_files, batch_size)

val_files = ["/content/NoteDetection-4/valid/notes.tfrecord"]
val_dataset = load_dataset(val_files, batch_size)

# Reproducability
def set_seed(seed=31415):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
set_seed()

# Set Matplotlib defaults
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('image', cmap='magma')
warnings.filterwarnings("ignore") # to clean up output cells

# Data Pipeline
'''The AUTOTUNE setting allows TensorFlow to dynamically adjust the prefetch buffer size at
runtime. This means TensorFlow will automatically determine the optimal number of data batches
to prefetch, based on available system resources and current workload.'''
def convert_to_float(image, label):
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return image, label

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_dataset = (
    train_dataset
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
val_dataset = (
    val_dataset
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)

print('Done')

from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    #layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same', input_shape=[256, 256, 3]),

    # Block One
    layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same',
                  input_shape=[224, 224, 3]),
    layers.MaxPool2D(),

    # Block Two
    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),
    layers.MaxPool2D(),

    # Block Three

    layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu', padding = 'same'),
    layers.Conv2D(filters = 128, kernel_size = 3, activation = 'relu', padding = 'same'),
    layers.MaxPool2D(),

    # Head
    layers.Flatten(),
    layers.Dense(6,'relu'),
    #layers.Dense(6, activation='softmax'), #using a new activation function: SoftMax **This is for multiclass**
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid'),
])



'''model.compile(optimizer='adam',
              loss='categorical_crossentropy',  # For multi-class classification
              metrics=['accuracy'])'''

#For Binary classification:
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),
    loss = 'binary_crossentropy',
    metrics = ['binary_accuracy'],
)

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=50,
)

